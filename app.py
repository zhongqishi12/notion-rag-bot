import os
import streamlit as st
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from openai import OpenAI

# ä½ å†™è¿‡çš„ DashScope Embeddings ç±»
from build_vector_db import DashScopeEmbeddings

load_dotenv()

# åˆå§‹åŒ–å‘é‡åº“
embeddings = DashScopeEmbeddings(model="text-embedding-v4", dimension=1024)
vs = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)

# åˆå§‹åŒ– LLM (é€šä¹‰ DashScope å…¼å®¹æ¨¡å¼)
client = OpenAI(
    api_key=os.getenv("DASHSCOPE_API_KEY"),
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
)

st.set_page_config(page_title="ğŸ“š Notion RAG Chatbot", layout="wide")
st.title("ğŸ“š æˆ‘çš„ç§äººçŸ¥è¯†åº“ Chatbot")

# ä¿æŒå¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±•ç¤ºå†å²æ¶ˆæ¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# è¾“å…¥æ¡†
if prompt := st.chat_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # å‘é‡æ£€ç´¢
    docs = vs.similarity_search(prompt, k=3)
    context = "\n\n".join([d.page_content for d in docs])

    # æ‹¼æ¥ Prompt
    full_prompt = f"""ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œè¯·ä»…åŸºäºä»¥ä¸‹å†…å®¹å›ç­”é—®é¢˜ã€‚
    å†…å®¹:
    {context}

    é—®é¢˜: {prompt}
    """

    # è°ƒç”¨ LLM
    resp = client.chat.completions.create(
        model="qwen-plus",
        messages=[{"role": "user", "content": full_prompt}],
    )

    answer = resp.choices[0].message.content
    sources = [d.metadata.get("source") for d in docs]

    # å±•ç¤ºåŠ©æ‰‹å›ç­”
    with st.chat_message("assistant"):
        st.markdown(answer)
        st.markdown(f"**æ¥æº:** {sources}")
    st.session_state.messages.append({"role": "assistant", "content": answer})